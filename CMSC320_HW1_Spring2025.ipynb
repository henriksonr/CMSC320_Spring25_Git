{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henriksonr/CMSC320_Spring25_Git/blob/main/CMSC320_HW1_Spring2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMEWORK 1: CAUTION! CONTENTS ARE HOT** üåã\n",
        "## **DUE: *FEBRUARY 20, 2025 @ 11:59 PM***\n",
        "## **24-HR LATE DUE DATE WITH A 15% PENALTY: *FEBRUARY 21, 2025 @ 11:59 PM***\n",
        "\n",
        "The [NCEI/WDS Global Significant Volcanic Eruptions Database](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.mgg.hazards:G10147) is a very comprehensive collection of +600 volcanic eruptions dating from 4360 BC to the present. Due to the nature of this assignment, we will be dealing with relatively newer volcanoes (in which some are still obviously still older than anyone on Earth currently). Each eruption in the database is classified as significant if it meets one or more criteria, such as causing fatalities, incurring **damage on property** (**+$1 million**), reaching a **Volcanic Explosivity Index (VEI)** of **6 or higher**, generating a tsunami, or being linked to a significant earthquake. The VEI is a scale that measures the explosiveness of volcanic eruptions, providing insight into the magnitude and potential consequences of the eruptions. The database includes detailed information on the location, type of volcano, last known eruption, VEI, casualties, property damage, and much more.\n",
        "![volcano](https://wikitravel.org/upload/shared//9/99/Volcano_de_Fuego_Banner.jpg)\n",
        "**We are going to dive straight into these volcanoes (well... their dataset), to swim our way into Pandas proficiency!**\n",
        "\n",
        "You will find the [Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html) helpful. There are also some helpful links to guide you along the way! Don't get burned üî•\n",
        "\n"
      ],
      "metadata": {
        "id": "FwALoygGD617"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DO NOT REMOVE ANY PART OF ANY OF THE QUESTIONS OR YOU LOSE CREDIT**\n",
        "### *No Hardcoding either*  üòã‚ù§Ô∏è‚Äçüî•\n",
        "### **REMEMBER TO SHOW ALL CODE OUTPUT (NO CREDIT OTHERWISE)**"
      ],
      "metadata": {
        "id": "4sQEC_lpHYdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1: Maintenance ü§© (25 POINTS TOTAL)**\n",
        "First, we're going to familiarize ourselves with the process. As in most languages, Python looks best when its modules are imported first before any other code is written ‚ú®"
      ],
      "metadata": {
        "id": "HrD3qZmJUp6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure these code blocks run properly and that you have properly installed the appropriate modules required.\n",
        "import pandas as pd\n",
        "import requests\n",
        "# import other libraries here\n",
        "\n",
        "# Don't remove this\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "CruzvUOrKEoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may have noticed, there's another library aside from Pandas called \"[requests](https://requests.readthedocs.io/en/latest/).\" **The requests library allows you to send HTTP requests to a server, retrieve the content, and process it at ease.** It's very beginner friendly for those attempting to get into webscraping (super important for collecting and creating datasets). We also recommend looking into [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) (yeah, soup LOL), another wonderful library that can be paired with the requests library for webscraping.\n",
        "\n",
        "As shown below, sometimes specific websites require specific headers in order to process a request to access the data.\n",
        "\n",
        "To check if a request was processed successfully, use the [status_code](https://requests.readthedocs.io/en/latest/api/) function to see if the process returned 200."
      ],
      "metadata": {
        "id": "7nWwbZ4jK5Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API URL and headers in case request gets denied.\n",
        "api_url = \"https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanoes\"\n",
        "\n",
        "headers = {\n",
        "    'accept': '*/*'\n",
        "}"
      ],
      "metadata": {
        "id": "MMxyFjVrKKvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.0: Cute Webscraping (5 points)**\n",
        "- To make our cute webscraper we need to **create a GET request** using the hints above.\n",
        "\n",
        "- This particular dataset NOAA returns data from the API as ***json*** when a user makes a request.\n",
        "- The json data has a particular format, so we will **extract our needed information only from the field called items** to make a dataframe (you may need to store this data before turning it into a dataframe).\n",
        "\n",
        "- After properly scraping the data, **name this dataframe** ***df***\n",
        "\n",
        "- Save this dataframe into a **CSV file named 'volcanoes.csv'**\n",
        "\n",
        "**You won't need to run this cell more than once**"
      ],
      "metadata": {
        "id": "zwJMQjBmJ5Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pRfxOpACC39n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.1: 1 Liner Thingz (3 points)**\n",
        "\n",
        "We need to get an idea of what this dataset is going to look. In order to do that, let's take a look at some of the most [basic things](https://dataanalytics.buffalostate.edu/pandas-cheat-sheet) our dataframe has.\n",
        "\n",
        "**Read the directions carefully and code your answer with only one line of code.**\n",
        "\n",
        "***CAN'T USE LOOPS. DO NOT DISPLAY THE DATAFRAME, JUST YOUR CODE OUTPUT HERE.***"
      ],
      "metadata": {
        "id": "geMtNQeiEo8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.1:** In one line of code and **using only one single attribute call**, output ***only the numbers*** of **datapoints and features** in the dataframe.\n",
        "\n",
        "*Hint: The output's going to be a tuple*"
      ],
      "metadata": {
        "id": "6Uyhc9VOJdbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "a_PPlwLyJbbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.2:** In one line of code and **using only one single attribute call**, list the **names** of ***all the features*** in the dataframe."
      ],
      "metadata": {
        "id": "OoMluxmiESrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "B4XwpjUUETbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't be using some of the data because there is a lot of missing data.\n",
        "\n",
        "**1.1.3:** *In one line of code, create a **new dataframe** called **new_df** that **discards** all the features of the **old** dataframe **except for the following**:*\n",
        "\n",
        "`id,\tyear, month, day,\ttsunamiEventId, earthquakeEventId, volcanoLocationId, volcanoLocationNewNum, name, country, elevation, morphology, deathsTotal, vei, deaths`\n",
        "*Hint: Don't use any drop function here*"
      ],
      "metadata": {
        "id": "6uvL0VpKNeAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "new_df # KEEP THIS. It will display the whole dataframe."
      ],
      "metadata": {
        "id": "rmFy8xLFLRrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.2: 1 Liner Shenaniganz (7 points)**\n",
        "\n",
        "We're going to tidy up the **new dataframe** a little more with some more advanced 1 liner code.\n",
        "\n",
        "**Read the directions carefully and code your answer with only one line of code.**\n",
        "\n",
        "**For this section, keep the method of display that is already in the box. Write your code as indicated.**\n",
        "\n",
        "***YOU CAN'T USE ONE LINE LOOPS OR ANY KIND OF LOOP.***"
      ],
      "metadata": {
        "id": "c8xP6O37UQp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.1:** *In one line of code and **using only one single function call**, **drop any row** that contains **NaN** in **any one** of the columns indicating a measure of **time**.*"
      ],
      "metadata": {
        "id": "I_SHOaZRIblA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "new_df # KEEP THIS. It will display the whole dataframe."
      ],
      "metadata": {
        "collapsed": true,
        "id": "B9qVE-OdGysV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.2:** *In one line of code, **change** the **index column** of the dataframe so that it has **1-based indexing**.*"
      ],
      "metadata": {
        "id": "OO-A8f3gGEpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "new_df # KEEP THIS. It will display the whole dataframe."
      ],
      "metadata": {
        "id": "gBBUKUJVFI0Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **deathsTotal** and **deaths**  columns have approximations of the same data with alternating NaNs in each.\n",
        "\n",
        "**1.2.3:** *In one line of code, make a **new column** called **'totalDeaths'** that takes the **max** of the values given between those* ***two*** *columns.\n",
        "- If there is **NaN** in ***one column*** and a **numerical** value in the **other**, it will ***take the numerical value***.\n",
        "- **Only** if there are **NaNs** in ***both columns***, the **new column will have NaN.**"
      ],
      "metadata": {
        "id": "oSxQRt-xio1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "new_df # KEEP THIS. It will display the whole dataframe."
      ],
      "metadata": {
        "id": "GEZ0YdSPdYyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TASK 1.3: Tailoring Time (10 Points)**\n",
        "It's pretty obvious that the year, month, and day look pretty weird in the dataset. We're going to have to do some hardcore cleaning on the [time](https://pandas.pydata.org/docs/user_guide/timeseries.html). We will learn more about data cleaning in class soon, but here we will perform some basic data cleaning.\n",
        "\n",
        "- **We need to have only ONE column called** \"***date***\" **that contains the full date in the following format YYYY-MM-DD, not separated into three columns.**\n",
        "\n",
        "- **Make sure there are no floating point values in the date.**\n",
        "- **Sort the data from most recent to least.**\n",
        "- **Remove the old columns and place the new column next to the 'id' column.**\n",
        "\n",
        "\n",
        "**YOU CAN USE MULTIPLE LINES OF CODE BUT CAN'T USE LOOPS OR HARDCODE.**\n",
        "\n",
        "**Note:** It is alright to have only a **maximum of 12 NaTs** for some dates that often go further back in time because the **datetime module** in Pandas has a year limit (unless otherwise guided)."
      ],
      "metadata": {
        "id": "9if9utm8jsdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "new_df # KEEP THIS. It will display the whole dataframe."
      ],
      "metadata": {
        "id": "Gs4GJqzgn2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: Volcanic Matryoshkas ü™Ü (30 POINTS TOTAL)**\n",
        "\n",
        "Now, that most of the data has been tidied up. We will organize the data into more sizable pieces of information in order to extract useful information.\n",
        "**You can use loops in the section if you wish, however your results must be displayed in a viewable manner.**"
      ],
      "metadata": {
        "id": "IlhYym73iOZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1:** *(10 points here)*\n",
        "\n",
        "Use the **groupby function in Pandas** to create separate dataframes for each unique country.\n",
        "\n",
        "* **Each table must only have the columns**: 'date' 'country', 'name', and 'vei'\n",
        "\n",
        "* **Sort** the dataframe of **each country** by **highest to lowest 'vei'**\n",
        "\n",
        "* Use the **[display](https://ipython.readthedocs.io/en/8.26.0/api/generated/IPython.display.html)** function to show **each sorted table**\n",
        "\n",
        "**You MUST use the groupby function here and display your results.**"
      ],
      "metadata": {
        "id": "5yLjy9u0VuhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "YonGiaib0noL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2:** *(5 points here)*\n",
        "\n",
        "Using **groupby again**, **print** out the **maximum 'vei'** for **each unique country.**\n",
        "\n",
        "**You MUST use the groupby function here and print your results.**\n",
        "\n",
        "* **Print** out your results in this format: \"Country: {country_name}, Highest VEI: {vei}\""
      ],
      "metadata": {
        "id": "OTExPGKLA7lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "DRHCvg1b2MEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have ALMOST REACHED THE END!!\n",
        "Since there is still quite a bit of missing data, we want to make use of what is still available.\n",
        "\n",
        "A very powerful tool in Python's magnificent collection of libraries is its beautiful graphing tools.\n",
        "\n",
        "Check out libraries such as [Seaborn](https://seaborn.pydata.org/) or [Matplotlib](https://matplotlib.org/stable/index.html) to create meaningful visualizations! **Your final task in this section requires the use of these libraries**\n",
        "\n",
        "**2.1.3:** *(15 points here)*\n",
        "\n",
        "- Based on the **unique names of volcanoes**, **filter names that have more than 3 datapoints under their name.**\n",
        "- Each datapoint in the dataframe refers to a recorded instance of a volcanic eruption.\n",
        "- Make **separate line graphs for each volcano** and **plot their VEIs over time.**\n",
        "- You **must display each graph** to receive credit.\n",
        "\n",
        "**Make sure to properly label all parts of the graph appropriately to receive credit üëÄ** (like title, axes, legend, etc...)"
      ],
      "metadata": {
        "id": "11rdFk2qCu8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "jvasG8eViN1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 3: Fiery Jobs üöí (15 POINTS TOTAL)**\n",
        "\n",
        "Proficiency in **SQL** is also super important. SQL databases are essentially relational databases in which there are vast amounts of tabular data. which can often be used to connect with related tablular data. [This](https://www.w3schools.com/sql/) is a pretty good intro into learning more about SQL."
      ],
      "metadata": {
        "id": "Gozs5JwKO7ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out this [tutorial](https://mode.com/sql-tutorial/introduction-to-sql/) for some clarifications on SQL.\n",
        "\n",
        "Now! We'll be using **`sqlite`** to access a database.\n",
        "* Start by downloading the sql lite file and putting it in the same directory as this [notebook](https://www.kaggle.com/datasets/kaggle/sf-salaries) (hit the 'download' button in the upper right).\n",
        "* Check out the description of the data so you know the table / column names.\n",
        "\n",
        "**The following code will use `sqlite3` to create a database connection.** `sqlite3` is the library in Python that assists in navigating through SQL databases.\n",
        "\n",
        "**Note:** If you are working on this assignment via Google Colab, sometimes the runtime resets and it will throw errors.\n",
        "\n",
        "***Instead of running through the entire notebook, run the notebook from the following code block and onwards:***\n",
        "- Click anywhere on the next code block.\n",
        "- Go up to where it says **'Runtime'** in the toolbar (right under the title of the notebook and **in between 'Insert' and 'Tools'**)\n",
        "- Hover over it and **click on the option** that says **'Run cell and below'**"
      ],
      "metadata": {
        "id": "bfrjZ3wrO-R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "conn = sqlite3.connect(\"database.sqlite\")\n",
        "crsr = conn.cursor()"
      ],
      "metadata": {
        "id": "ooUWEVfYPAkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will let you check out the different tables within the database.\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = crsr.execute(query).fetchall()\n",
        "print(tables)"
      ],
      "metadata": {
        "id": "pnJiNMw7o9d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87598252-9a6a-4ea9-983c-cbe91a61eb0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Salaries',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember that each problem should be solved with a single SQL query.**\n",
        "**Note: All outputs must be shown**\n",
        "- Only include whatever fields are mentioned throughout each question, nothing more and nothing less.\n",
        "- Follow each instruction clearly\n",
        "\n",
        "#### **3.1.1: 2 Points**\n",
        "From the **Salaries** table, get the **average base pay** for firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**) between the **years 2012 to 2014**.\n",
        "\n",
        "*Remember that firefighters that also occupy other professions are still considered firefighters.*\n",
        "\n",
        "*Hint: Look into [this](https://www.w3schools.com/mysql/mysql_wildcards.asp) üëÄ*"
      ],
      "metadata": {
        "id": "KutbMSr2pV-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "3wm6NlYqp3dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.2: 2 Points**\n",
        "From the **Salaries** table, get all the firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**) in the **year 2012** making under **$90,000 as a base pay.** **Sort** them in **descending** order by their pay.\n",
        "\n",
        "*Remember that firefighters that also occupy other professions are still considered firefighters.*"
      ],
      "metadata": {
        "id": "AwWzAyYbq9yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "ZQ_hK1tpq83U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.3: 4 Points**\n",
        "From the **Salaries** table, first get the **averages** of **base pay**, **benefits**, and **overtime pay** for firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**).\n",
        "\n",
        "- Then, make a **column with the sum** of these **three averages**\n",
        "- Finally, **exclude** job titles containing \"FIREFIGHTER\" **(case-sensitive)**\n",
        "\n",
        "*Remember that firefighters that also occupy other professions are still considered firefighters.*"
      ],
      "metadata": {
        "id": "zNRnMM7Cq9gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'REMOVE THIS CONTENT AND ANSWER IN HERE'\n",
        "\n",
        "# KEEP THIS. It will display the whole dataframe.\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ],
      "metadata": {
        "id": "yVOCgk2Jq9LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.4: 7 Points**\n",
        "\n",
        "Finally, we'll make our own table in our database.\n",
        "\n",
        "* Separate the **Salaries table** by **years**, and add it back to the database.\n",
        "- Using a loop might be helpful.\n",
        "\n",
        "* You may use basic python to complete the task. However, using querying on SQL is **mandatory**.\n",
        "* Feel free to **use multiple lines of code for this problem only.**\n",
        "\n",
        "*Hint: Check [this](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) out*"
      ],
      "metadata": {
        "id": "0cp88tfxq-Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THIS CONTENT AND ANSWER IN YOUR OWN WAY"
      ],
      "metadata": {
        "id": "YjQnj4Uqq9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to check if you successfully added your table.\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())"
      ],
      "metadata": {
        "id": "RYcadk9g4jE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ddcf42-34b5-4dd6-8093-a133ba878c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Salaries',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![volcano](https://as1.ftcdn.net/v2/jpg/06/34/76/64/1000_F_634766457_0fZbpYj6aBLlldO1jADUPpKTRLnNmngs.jpg)"
      ],
      "metadata": {
        "id": "oW-M2Epk28g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: BONUS SECTION (Pandas 'Group By')\n",
        "\n"
      ],
      "metadata": {
        "id": "5FnP5lBUbor4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This flowchart is taken from our lecture class presentation and illustrates the process of transforming data using the Pandas GroupBy operation. First, the data is input, followed by applying the GroupBy function to one or more columns of the DataFrame. Once the data is grouped, an aggregation function (such as sum(), mean(), or count()) is applied to compute summary statistics for each group\n",
        "\n",
        "![bonus](https://drive.google.com/uc?export=view&id=1OeO3cVJSgmk6QxeBUp5ojJQUGVAPB-Jx)\n",
        "\n"
      ],
      "metadata": {
        "id": "01n5h_yLbrNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to translate the workflow shown in the flowchart into Pandas queries that perform these operations step by step.\n",
        "\n",
        "**Notes:** Your task is to translate the workflow shown in the flowchart into Pandas queries. Ensure that the **exact input and exact output** from the flowchart are replicated using Pandas queries, step by step."
      ],
      "metadata": {
        "id": "kY3rAavEb1WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 (Point 1 )**\n",
        "\n",
        "Create a **sample dataset** that includes columns for ***account, order,*** and ***ext price***."
      ],
      "metadata": {
        "id": "jrbuE9mCbyS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Sample dataset from above flowchart\n",
        "input_data = {\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# Display Dataframe (DONT REMOVE THE CODE)\n",
        "df"
      ],
      "metadata": {
        "id": "gL9RNpdFb2iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 (Point 1+1 =2 )**\n",
        "\n",
        "Group by **order** and **show** the **intermediate results**"
      ],
      "metadata": {
        "id": "eBXFEjDOb5CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'order' and show intermediate result\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# Display intermediate result for each group; hints: you have to use 'for loop'\n",
        "print(\"\\nIntermediate Grouped Data (Before Aggregation):\")\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE"
      ],
      "metadata": {
        "id": "3wbXWgI1b7ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 (Point 1 )** Apply the **Sum Aggregation** for **Each** Group\n",
        "\n",
        "Now we'll apply the sum aggregation to get the ***total ext price*** for ***each order***:"
      ],
      "metadata": {
        "id": "Ci0Ilh3qb_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat group by 'order' again and then apply aggregation (sum of 'ext_price' for each 'order')\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# Show the aggregated result after re-grouping  (DONT REMOVE THE CODE)\n",
        "print(\"\\nAggregated Data (Sum of 'ext_price' per 'order'):\")\n",
        "print(aggregated_result)"
      ],
      "metadata": {
        "id": "DR0cBUhCcCWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 (Point 1)**  Combine the Results into One Final Table.\n",
        "\n",
        "Finally, we will **reset the index** and create a combined table that shows order and the sum of the ext price for each group:\n",
        "\n",
        "\n",
        "**Notes**: In pandas, `reset_index()` is a method used to reset the index of a DataFrame to its default integer-based index. By default, when you perform certain operations like `groupby()`, the resulting DataFrame may have a new index (e.g., the grouped column). The `reset_index()` method allows you to convert the current index back to a default sequential integer index and optionally, move the current index values into a regular column.\n"
      ],
      "metadata": {
        "id": "Ch3OfDGJcEuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset index to combine result into a single DataFrame\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# Rename the columns for clarity\n",
        "\n",
        "# REMOVE THIS COMMENT AND ANSWER STARTING HERE\n",
        "\n",
        "# Show the final result  (DONT REMOVE THE CODE)\n",
        "print(\"\\nFinal Combined Result (Order and Total 'ext_price'):\")\n",
        "print(final_result)"
      ],
      "metadata": {
        "id": "qlso8XUOpgZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE END!**"
      ],
      "metadata": {
        "id": "aiTkGTy8cJrG"
      }
    }
  ]
}